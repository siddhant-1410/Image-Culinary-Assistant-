{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Paths for your datasets\n",
        "train_dir = r\"/content/dataset_testing_final/train\"\n",
        "val_dir = r\"/content/dataset_testing_final/valid\"\n",
        "\n",
        "# Count images per class for the training data\n",
        "class_counts = {}\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        count = len([fname for fname in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, fname))])\n",
        "        class_counts[class_name] = count\n",
        "\n",
        "# Plot the class counts as a bar graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.title('Number of Images per Class in Training Data')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Define ImageDataGenerator for training with data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,  # Increased rotation\n",
        "    width_shift_range=0.3,  # Increased shift\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,  # Increased shear\n",
        "    zoom_range=0.3,  # Increased zoom\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True  # Added vertical flip\n",
        ")\n",
        "\n",
        "# Define ImageDataGenerator for validation (only rescaling)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    # First block\n",
        "    Conv2D(32, (3,3), padding='same', input_shape=(128, 128, 3)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, (3,3), padding='same'),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Second block\n",
        "    Conv2D(64, (3,3), padding='same'),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3,3), padding='same'),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Third block\n",
        "    Conv2D(128, (3,3), padding='same'),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(128, (3,3), padding='same'),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Fourth block (new)\n",
        "    Conv2D(256, (3,3), padding='same'),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(256, (3,3), padding='same'),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # Global pooling layer\n",
        "    GlobalAveragePooling2D(),\n",
        "\n",
        "    # Fully connected layers\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.3),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),  # Increased patience\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)  # Increased patience\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,  # Increased epochs\n",
        "    validation_data=val_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights_dict  # Use class weights\n",
        ")\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eTTW78fAy5c-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}